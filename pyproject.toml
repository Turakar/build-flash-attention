[project]
name = "build-flash-attention"
version = "0.1.0"
description = "Build flash-attn wheels"
requires-python = ">=3.12"
dependencies = [
    "ninja>=1.11.1.3",
    "numpy>=2.2.3",
    "packaging>=24.2",
    "pip>=25.0.1",
    "torch>=2.6.0",
]

[tool.uv.sources]
torch = { index = "pytorch_cu126", marker = "platform_machine == 'aarch64'" }

[[tool.uv.index]]
name = "pytorch_cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true
