[project]
name = "build-flash-attention"
version = "0.1.0"
description = "Build flash-attn wheels"
requires-python = ">=3.12"
dependencies = [
    "ninja>=1.11.1.3",
    "numpy>=2.2.3",
    #"nvidia-cublas-cu12==12.6.*",
    #"nvidia-cuda-cupti-cu12==12.6.*",
    #"nvidia-cuda-nvcc-cu12==12.6.*",
    #"nvidia-cuda-nvrtc-cu12==12.6.*",
    #"nvidia-cuda-runtime-cu12==12.6.*",
    #"nvidia-cudnn-cu12>=9.7.1.26",
    #"nvidia-cufft-cu12>=11.3.3.41",
    #"nvidia-curand-cu12>=10.3.9.55",
    #"nvidia-cusolver-cu12>=11.7.2.55",
    #"nvidia-cusparselt-cu12>=0.7.0",
    #"nvidia-nccl-cu12>=2.25.1",
    #"nvidia-nvjitlink-cu12==12.6.*",
    #"nvidia-nvtx-cu12==12.6.*",
    "packaging>=24.2",
    "pip>=25.0.1",
    "triton>=3.2.0",
]

[project.optional-dependencies]
torch_pypi = [
    "torch>=2.6.0",
]
torch_cu126 = [
    "torch>=2.6.0",
]

[tool.uv]
conflicts = [
    [ { extra = "torch_pypi" }, { extra = "torch_cu126" } ],
]

[tool.uv.sources]
torch = [
    { index = "pypi", extra = "torch_pypi" },
    { index = "pytorch_cu126", extra = "torch_cu126" },
]

[[tool.uv.index]]
name = "pypi"
url = "https://pypi.org/simple"
default = true

[[tool.uv.index]]
name = "pytorch_cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true
