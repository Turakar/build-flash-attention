[project]
name = "build-flash-attention"
version = "0.1.0"
description = "Build flash-attn wheels"
requires-python = ">=3.12"
dependencies = [
    "ninja>=1.11.1.3",
    "numpy>=2.2.3",
    "packaging>=24.2",
    "pip>=25.0.1",
    "torch==2.7.0",
    "triton",
]

[tool.uv.sources]
torch = { index = "pytorch_cu126", marker = "platform_machine == 'aarch64'" }
triton = { git = "https://github.com/triton-lang/triton", subdirectory = "python", rev = "96316ce50fade7e209553aba4898cd9b82aab83b" }

[[tool.uv.index]]
name = "pytorch_cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true
